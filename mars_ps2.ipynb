{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95139d43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Automated Meta Data Generation\n",
    "\n",
    "This notebook provides a comprehensive document analysis system that can:\n",
    "\n",
    "* Process various document formats (PDF, DOCX, TXT)\n",
    "* Perform advanced NLP analysis\n",
    "* Generate structured metadata\n",
    "* Export results in multiple formats\n",
    "\n",
    "\n",
    "\n",
    "### Features\n",
    "\n",
    "* **Document Processing**: PDF, DOCX, TXT with OCR support\n",
    "* **NLP Analysis**: Entity extraction, topic detection, sentiment analysis\n",
    "* **Metadata Generation**: Structured document metadata\n",
    "* **Export Options**: JSON, XML, YAML formats\n",
    "* **Batch Processing**: Process multiple documents at once\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8809fe0",
   "metadata": {},
   "source": [
    "### Installation Requirements\n",
    "\n",
    "Run this cell to install all required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f876af2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: textstat in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (0.7.7)\n",
      "Requirement already satisfied: langdetect in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from python-docx) (4.14.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: pyphen in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: six in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kmani\\onedrive\\documents\\metadata_gen_mars\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 9.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.7/12.8 MB 3.1 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.1 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.8/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "Could not find platform independent libraries <prefix>\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install PyPDF2 python-docx pytesseract Pillow spacy textstat langdetect wordcloud matplotlib seaborn pandas numpy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1c141ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ðŸ“Š SpaCy available: True\n",
      "ðŸ” OCR available: True\n"
     ]
    }
   ],
   "source": [
    "## Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "import logging\n",
    "import pdfplumber\n",
    "import unicodedata\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "from docx import Document as DocxDocument\n",
    "try:\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    OCR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OCR_AVAILABLE = False\n",
    "\n",
    "# NLP libraries\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    SPACY_AVAILABLE = True\n",
    "except (ImportError, OSError):\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "import textstat\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š SpaCy available: {SPACY_AVAILABLE}\")\n",
    "print(f\"ðŸ” OCR available: {OCR_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0c5fa",
   "metadata": {},
   "source": [
    "### Document Processor Class\n",
    "\n",
    "- Handles document processing for various file formats including PDF, DOCX, and TXT.  \n",
    "- Includes OCR capabilities for image-based documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f1e85d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive document processor that extracts text from various formats\n",
    "    and prepares it for metadata generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tesseract_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the document processor.\n",
    "        \n",
    "        Args:\n",
    "            tesseract_path: Path to tesseract executable (if not in PATH)\n",
    "        \"\"\"\n",
    "        self.supported_formats = ['.pdf', '.docx', '.txt', '.doc']\n",
    "        \n",
    "        # Set tesseract path if provided\n",
    "        if tesseract_path:\n",
    "            pytesseract.pytesseract.tesseract_cmd = tesseract_path\n",
    "        \n",
    "        # Test OCR availability\n",
    "        self._test_ocr()\n",
    "    \n",
    "    def _test_ocr(self) -> bool:\n",
    "        \"\"\"Test if OCR is working properly.\"\"\"\n",
    "        try:\n",
    "            pytesseract.get_tesseract_version()\n",
    "            logger.info(\"OCR (Tesseract) is available\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"OCR not available: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_document(self, file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a document and extract text content with metadata.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing extracted text and basic metadata\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        file_path = Path(file_path)\n",
    "        file_extension = file_path.suffix.lower()\n",
    "        \n",
    "        if file_extension not in self.supported_formats:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "        \n",
    "        # Basic file metadata\n",
    "        file_stats = file_path.stat()\n",
    "        basic_metadata = {\n",
    "            'filename': file_path.name,\n",
    "            'file_size': file_stats.st_size,\n",
    "            'file_extension': file_extension,\n",
    "            'file_path': str(file_path)\n",
    "        }\n",
    "        \n",
    "        # Extract text based on file type\n",
    "        try:\n",
    "            if file_extension == '.pdf':\n",
    "                text_content = self._process_pdf(file_path)\n",
    "            elif file_extension in ['.docx', '.doc']:\n",
    "                text_content = self._process_docx(file_path)\n",
    "            elif file_extension == '.txt':\n",
    "                text_content = self._process_txt(file_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Handler not implemented for {file_extension}\")\n",
    "            \n",
    "            # Process and clean text\n",
    "            processed_text = self._clean_text(text_content)\n",
    "            \n",
    "            # Basic text statistics\n",
    "            text_stats = self._get_text_statistics(processed_text)\n",
    "            \n",
    "            return {\n",
    "                'raw_text': text_content,\n",
    "                'processed_text': processed_text,\n",
    "                'file_metadata': basic_metadata,\n",
    "                'text_statistics': text_stats,\n",
    "                'processing_status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path}: {e}\")\n",
    "            return {\n",
    "                'raw_text': '',\n",
    "                'processed_text': '',\n",
    "                'file_metadata': basic_metadata,\n",
    "                'text_statistics': {},\n",
    "                'processing_status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def _process_pdf(self, file_path: Path) -> str:\n",
    "        \"\"\"Extract text from PDF files with OCR fallback.\"\"\"\n",
    "        text_content = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Try pdfplumber first (better for structured PDFs)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text_content += page_text + \"\\n\"\n",
    "            \n",
    "            # If we got very little text, try OCR\n",
    "            if len(text_content.strip()) < 100:\n",
    "                logger.info(f\"Low text extraction from {file_path.name}, trying OCR...\")\n",
    "                text_content = self._process_pdf_with_ocr(file_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"pdfplumber failed for {file_path.name}: {e}\")\n",
    "            # Fallback to PyPDF2\n",
    "            try:\n",
    "                text_content = self._process_pdf_pypdf2(file_path)\n",
    "            except Exception as e2:\n",
    "                logger.warning(f\"PyPDF2 also failed: {e2}\")\n",
    "                # Last resort: OCR\n",
    "                text_content = self._process_pdf_with_ocr(file_path)\n",
    "        \n",
    "        return text_content\n",
    "    \n",
    "    def _process_pdf_pypdf2(self, file_path: Path) -> str:\n",
    "        \"\"\"Fallback PDF processing with PyPDF2.\"\"\"\n",
    "        text_content = \"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text_content += page.extract_text() + \"\\n\"\n",
    "        return text_content\n",
    "    \n",
    "    def _process_pdf_with_ocr(self, file_path: Path) -> str:\n",
    "        \"\"\"Process PDF using OCR for scanned documents.\"\"\"\n",
    "        try:\n",
    "            text_content = \"\"\n",
    "            pdf_document = fitz.open(file_path)\n",
    "            \n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                page = pdf_document[page_num]\n",
    "                \n",
    "                # Convert page to image\n",
    "                pix = page.get_pixmap()\n",
    "                img_data = pix.tobytes(\"ppm\")\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                \n",
    "                # OCR the image\n",
    "                page_text = pytesseract.image_to_string(img)\n",
    "                text_content += page_text + \"\\n\"\n",
    "            \n",
    "            pdf_document.close()\n",
    "            return text_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"OCR processing failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _process_docx(self, file_path: Path) -> str:\n",
    "        \"\"\"Extract text from DOCX files.\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            text_content = \"\"\n",
    "            \n",
    "            # Extract paragraphs\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text_content += paragraph.text + \"\\n\"\n",
    "            \n",
    "            # Extract tables\n",
    "            for table in doc.tables:\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        text_content += cell.text + \" \"\n",
    "                    text_content += \"\\n\"\n",
    "            \n",
    "            return text_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing DOCX {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _process_txt(self, file_path: Path) -> str:\n",
    "        \"\"\"Process plain text files.\"\"\"\n",
    "        try:\n",
    "            # Try different encodings\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        return file.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # If all encodings fail, read as binary and decode with errors='ignore'\n",
    "            with open(file_path, 'rb') as file:\n",
    "                return file.read().decode('utf-8', errors='ignore')\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing TXT {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize extracted text.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _get_text_statistics(self, text: str) -> Dict:\n",
    "        \"\"\"Generate basic statistics about the extracted text.\"\"\"\n",
    "        if not text:\n",
    "            return {}\n",
    "        \n",
    "        # Basic counts\n",
    "        char_count = len(text)\n",
    "        word_count = len(text.split())\n",
    "        sentence_count = len([s for s in text.split('.') if s.strip()])\n",
    "        paragraph_count = len([p for p in text.split('\\n') if p.strip()])\n",
    "        \n",
    "        # Language detection\n",
    "        try:\n",
    "            blob = TextBlob(text[:1000])  # Use first 1000 chars for language detection\n",
    "            language = blob.detect_language()\n",
    "        except:\n",
    "            language = 'unknown'\n",
    "        \n",
    "        # Reading time estimation (average 200 words per minute)\n",
    "        reading_time_minutes = max(1, round(word_count / 200))\n",
    "        \n",
    "        return {\n",
    "            'character_count': char_count,\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'paragraph_count': paragraph_count,\n",
    "            'detected_language': language,\n",
    "            'estimated_reading_time_minutes': reading_time_minutes\n",
    "        }\n",
    "    \n",
    "    def batch_process(self, directory_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process multiple documents in a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory containing documents\n",
    "            \n",
    "        Returns:\n",
    "            List of processing results for each document\n",
    "        \"\"\"\n",
    "        if not os.path.exists(directory_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "        \n",
    "        directory = Path(directory_path)\n",
    "        results = []\n",
    "        \n",
    "        # Find all supported files\n",
    "        for file_path in directory.iterdir():\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.supported_formats:\n",
    "                logger.info(f\"Processing: {file_path.name}\")\n",
    "                result = self.process_document(str(file_path))\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d6634",
   "metadata": {},
   "source": [
    "### NLP Analyzer Class\n",
    "\n",
    "- Advanced NLP analysis for document metadata generation\n",
    "- Uses rule-based and statistical approaches for robust analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb9cea3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class EntityInfo:\n",
    "    \"\"\"Information about extracted entities\"\"\"\n",
    "    text: str\n",
    "    entity_type: str\n",
    "    confidence: float = 1.0\n",
    "    context: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class TopicInfo:\n",
    "    \"\"\"Information about extracted topics\"\"\"\n",
    "    topic: str\n",
    "    keywords: List[str]\n",
    "    confidence: float\n",
    "    sentences: List[str]\n",
    "\n",
    "class NLPAnalyzer:\n",
    "    \"\"\"\n",
    "    Advanced NLP analysis for document metadata generation\n",
    "    Uses rule-based and statistical approaches for robust analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the NLP analyzer with patterns and vocabularies\"\"\"\n",
    "        logger.info(\"Initializing NLP Analyzer...\")\n",
    "        \n",
    "        # Entity recognition patterns\n",
    "        self.entity_patterns = {\n",
    "            'email': re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'),\n",
    "            'phone': re.compile(r'(\\+\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'),\n",
    "            'url': re.compile(r'https?://(?:[-\\w.])+(?:\\:[0-9]+)?(?:/(?:[\\w/_.])*(?:\\?(?:[\\w&=%.])*)?(?:\\#(?:[\\w.])*)?)?'),\n",
    "            'date': re.compile(r'\\b(?:\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{2}-\\d{2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2},? \\d{4})\\b', re.IGNORECASE),\n",
    "            'currency': re.compile(r'\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})? (?:USD|EUR|GBP|CAD)\\b'),\n",
    "            'percentage': re.compile(r'\\b\\d+(?:\\.\\d+)?%\\b'),\n",
    "            'organization': re.compile(r'\\b(?:Inc|LLC|Corp|Ltd|Company|Organization|University|Institute|Department|Agency|Foundation|Association)\\b', re.IGNORECASE),\n",
    "        }\n",
    "        \n",
    "        # Common stop words for topic extraction\n",
    "        self.stop_words = {\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "            'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',\n",
    "            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your',\n",
    "            'his', 'her', 'its', 'our', 'their', 'myself', 'yourself', 'himself', 'herself', 'itself',\n",
    "            'ourselves', 'yourselves', 'themselves', 'what', 'which', 'who', 'whom', 'whose', 'where',\n",
    "            'when', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'just'\n",
    "        }\n",
    "        \n",
    "        # Technical/domain keywords that indicate topics\n",
    "        self.domain_keywords = {\n",
    "            'technology': ['software', 'hardware', 'computer', 'system', 'application', 'program', 'code', 'algorithm', 'data', 'database', 'network', 'server', 'cloud', 'api', 'framework', 'platform'],\n",
    "            'business': ['revenue', 'profit', 'sales', 'marketing', 'customer', 'client', 'market', 'strategy', 'business', 'company', 'organization', 'management', 'finance', 'budget', 'cost'],\n",
    "            'research': ['study', 'research', 'analysis', 'experiment', 'hypothesis', 'methodology', 'results', 'conclusion', 'findings', 'data', 'sample', 'statistical', 'significant'],\n",
    "            'medical': ['patient', 'treatment', 'diagnosis', 'medical', 'health', 'clinical', 'therapy', 'disease', 'symptom', 'medicine', 'doctor', 'hospital', 'healthcare'],\n",
    "            'legal': ['contract', 'agreement', 'legal', 'law', 'court', 'case', 'plaintiff', 'defendant', 'attorney', 'lawyer', 'litigation', 'settlement', 'regulation', 'compliance'],\n",
    "            'education': ['student', 'teacher', 'education', 'learning', 'course', 'curriculum', 'school', 'university', 'academic', 'degree', 'diploma', 'graduate', 'undergraduate'],\n",
    "            'finance': ['investment', 'portfolio', 'stock', 'bond', 'asset', 'liability', 'equity', 'dividend', 'interest', 'loan', 'credit', 'debt', 'financial', 'banking']\n",
    "        }\n",
    "        \n",
    "        logger.info(\"âœ… NLP Analyzer initialized!\")\n",
    "    \n",
    "    def analyze_document(self, text: str, filename: str = \"\") -> Dict:\n",
    "        \"\"\"\n",
    "        Perform comprehensive NLP analysis on document text\n",
    "        \n",
    "        Args:\n",
    "            text: Document text to analyze\n",
    "            filename: Optional filename for context\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing all analysis results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting NLP analysis for: {filename or 'text input'}\")\n",
    "        \n",
    "        # Clean and preprocess text\n",
    "        clean_text = self._preprocess_text(text)\n",
    "        sentences = self._split_sentences(clean_text)\n",
    "        words = self._extract_words(clean_text)\n",
    "        \n",
    "        # Perform various analyses\n",
    "        analysis_results = {\n",
    "            'entities': self._extract_entities(text),\n",
    "            'topics': self._extract_topics(clean_text, sentences, words),\n",
    "            'summary': self._generate_summary(sentences),\n",
    "            'keywords': self._extract_keywords(words),\n",
    "            'document_structure': self._analyze_structure(text),\n",
    "            'sentiment': self._analyze_sentiment(clean_text),\n",
    "            'readability': self._analyze_readability(sentences, words),\n",
    "            'language_features': self._analyze_language_features(text, words)\n",
    "        }\n",
    "        \n",
    "        logger.info(\"âœ… NLP analysis completed\")\n",
    "        return analysis_results\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text for analysis\"\"\"\n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        # Remove excessive whitespace but preserve paragraph structure\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using rule-based approach\"\"\"\n",
    "        # Simple sentence splitting - handles most cases\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _extract_words(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract and clean words from text\"\"\"\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        return [word for word in words if len(word) > 2]\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> Dict[str, List[EntityInfo]]:\n",
    "        \"\"\"Extract named entities using pattern matching\"\"\"\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for entity_type, pattern in self.entity_patterns.items():\n",
    "            matches = pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                entity_text = match.group().strip()\n",
    "                if entity_text:  # Avoid empty matches\n",
    "                    # Get context (20 characters before and after)\n",
    "                    start = max(0, match.start() - 20)\n",
    "                    end = min(len(text), match.end() + 20)\n",
    "                    context = text[start:end].replace('\\n', ' ')\n",
    "                    \n",
    "                    entities[entity_type].append(EntityInfo(\n",
    "                        text=entity_text,\n",
    "                        entity_type=entity_type,\n",
    "                        confidence=0.9,  # High confidence for pattern matches\n",
    "                        context=context\n",
    "                    ))\n",
    "        \n",
    "        # Remove duplicates\n",
    "        for entity_type in entities:\n",
    "            seen = set()\n",
    "            unique_entities = []\n",
    "            for entity in entities[entity_type]:\n",
    "                if entity.text.lower() not in seen:\n",
    "                    seen.add(entity.text.lower())\n",
    "                    unique_entities.append(entity)\n",
    "            entities[entity_type] = unique_entities\n",
    "        \n",
    "        return dict(entities)\n",
    "    \n",
    "    def _extract_topics(self, text: str, sentences: List[str], words: List[str]) -> List[TopicInfo]:\n",
    "        \"\"\"Extract main topics using keyword analysis and domain detection\"\"\"\n",
    "        topics = []\n",
    "        \n",
    "        # Count word frequencies (excluding stop words)\n",
    "        word_freq = Counter([word for word in words if word not in self.stop_words])\n",
    "        \n",
    "        # Identify domain-specific topics\n",
    "        domain_scores = defaultdict(int)\n",
    "        domain_keywords_found = defaultdict(list)\n",
    "        \n",
    "        for domain, keywords in self.domain_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in word_freq:\n",
    "                    domain_scores[domain] += word_freq[keyword]\n",
    "                    domain_keywords_found[domain].append(keyword)\n",
    "        \n",
    "        # Create topic info for top domains\n",
    "        for domain, score in sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "            if score > 2:  # Minimum threshold\n",
    "                relevant_sentences = []\n",
    "                for sentence in sentences[:10]:  # Check first 10 sentences\n",
    "                    if any(keyword in sentence.lower() for keyword in domain_keywords_found[domain]):\n",
    "                        relevant_sentences.append(sentence)\n",
    "                \n",
    "                topics.append(TopicInfo(\n",
    "                    topic=domain.title(),\n",
    "                    keywords=domain_keywords_found[domain][:5],\n",
    "                    confidence=min(score / 10, 1.0),\n",
    "                    sentences=relevant_sentences[:3]\n",
    "                ))\n",
    "        \n",
    "        # Add general high-frequency topics\n",
    "        top_words = [word for word, count in word_freq.most_common(10) if count > 2]\n",
    "        if top_words:\n",
    "            topics.append(TopicInfo(\n",
    "                topic=\"General\",\n",
    "                keywords=top_words[:5],\n",
    "                confidence=0.5,\n",
    "                sentences=sentences[:2]\n",
    "            ))\n",
    "        \n",
    "        return topics\n",
    "    \n",
    "    def _extract_keywords(self, words: List[str]) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Extract important keywords with frequency\"\"\"\n",
    "        word_freq = Counter([word for word in words if word not in self.stop_words])\n",
    "        \n",
    "        # Filter out very common and very rare words\n",
    "        total_words = len(words)\n",
    "        keywords = []\n",
    "        \n",
    "        for word, freq in word_freq.most_common(20):\n",
    "            # Skip if too rare (< 0.1%) or too common (> 5%)\n",
    "            percentage = freq / total_words\n",
    "            if 0.001 <= percentage <= 0.05 and len(word) > 3:\n",
    "                keywords.append((word, freq))\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def _generate_summary(self, sentences: List[str], max_sentences: int = 3) -> str:\n",
    "        \"\"\"Generate extractive summary using sentence scoring\"\"\"\n",
    "        if not sentences:\n",
    "            return \"\"\n",
    "        \n",
    "        if len(sentences) <= max_sentences:\n",
    "            return \" \".join(sentences)\n",
    "        \n",
    "        # Score sentences based on length and position\n",
    "        sentence_scores = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            \n",
    "            # Position score (earlier sentences are more important)\n",
    "            position_score = 1.0 - (i / len(sentences))\n",
    "            score += position_score * 0.3\n",
    "            \n",
    "            # Length score (medium length sentences preferred)\n",
    "            word_count = len(sentence.split())\n",
    "            if 10 <= word_count <= 30:\n",
    "                score += 0.4\n",
    "            elif 5 <= word_count < 10 or 30 < word_count <= 50:\n",
    "                score += 0.2\n",
    "            \n",
    "            # Keyword score - sentences with important words\n",
    "            important_words = ['important', 'significant', 'key', 'main', 'primary', 'conclusion', 'result']\n",
    "            if any(word in sentence.lower() for word in important_words):\n",
    "                score += 0.3\n",
    "            \n",
    "            sentence_scores.append((sentence, score))\n",
    "        \n",
    "        # Select top sentences\n",
    "        top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:max_sentences]\n",
    "        \n",
    "        # Return in original order\n",
    "        selected_indices = []\n",
    "        for sentence, _ in top_sentences:\n",
    "            try:\n",
    "                idx = sentences.index(sentence)\n",
    "                selected_indices.append(idx)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        selected_indices.sort()\n",
    "        summary_sentences = [sentences[i] for i in selected_indices]\n",
    "        \n",
    "        return \" \".join(summary_sentences)\n",
    "    \n",
    "    def _analyze_structure(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze document structure\"\"\"\n",
    "        structure = {\n",
    "            'has_title': False,\n",
    "            'has_headings': False,\n",
    "            'has_lists': False,\n",
    "            'has_tables': False,\n",
    "            'sections': 0,\n",
    "            'paragraphs': len(text.split('\\n\\n'))\n",
    "        }\n",
    "        \n",
    "        # Check for title (first line in caps or with specific patterns)\n",
    "        lines = text.split('\\n')\n",
    "        if lines:\n",
    "            first_line = lines[0].strip()\n",
    "            if (first_line.isupper() or \n",
    "                any(char in first_line for char in [':', '=', '-']) and len(first_line) < 100):\n",
    "                structure['has_title'] = True\n",
    "        \n",
    "        # Check for headings (lines with specific patterns)\n",
    "        heading_patterns = [\n",
    "            r'^[A-Z][A-Za-z\\s]+:$',  # \"Section Name:\"\n",
    "            r'^#+\\s',                 # Markdown headers\n",
    "            r'^[IVX]+\\.',            # Roman numerals\n",
    "            r'^\\d+\\.',               # Numbered sections\n",
    "        ]\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(re.match(pattern, line) for pattern in heading_patterns):\n",
    "                structure['has_headings'] = True\n",
    "                structure['sections'] += 1\n",
    "        \n",
    "        # Check for lists\n",
    "        if re.search(r'^\\s*[-*â€¢]\\s', text, re.MULTILINE):\n",
    "            structure['has_lists'] = True\n",
    "        \n",
    "        # Check for tables (simple heuristic)\n",
    "        if '|' in text or '\\t' in text:\n",
    "            structure['has_tables'] = True\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def _analyze_sentiment(self, text: str) -> Dict:\n",
    "        \"\"\"Basic sentiment analysis using word lists\"\"\"\n",
    "        positive_words = {\n",
    "            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'outstanding',\n",
    "            'positive', 'success', 'successful', 'achieve', 'achievement', 'improve', 'improvement',\n",
    "            'benefit', 'advantage', 'effective', 'efficient', 'valuable', 'important', 'significant'\n",
    "        }\n",
    "        \n",
    "        negative_words = {\n",
    "            'bad', 'terrible', 'awful', 'horrible', 'poor', 'negative', 'fail', 'failure',\n",
    "            'problem', 'issue', 'challenge', 'difficult', 'hard', 'impossible', 'wrong',\n",
    "            'error', 'mistake', 'concern', 'risk', 'threat', 'danger', 'loss', 'decline'\n",
    "        }\n",
    "        \n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        \n",
    "        positive_count = sum(1 for word in words if word in positive_words)\n",
    "        negative_count = sum(1 for word in words if word in negative_words)\n",
    "        total_sentiment_words = positive_count + negative_count\n",
    "        \n",
    "        if total_sentiment_words == 0:\n",
    "            sentiment = \"neutral\"\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = (positive_count - negative_count) / total_sentiment_words\n",
    "            if score > 0.1:\n",
    "                sentiment = \"positive\"\n",
    "            elif score < -0.1:\n",
    "                sentiment = \"negative\"\n",
    "            else:\n",
    "                sentiment = \"neutral\"\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'score': score,\n",
    "            'positive_words': positive_count,\n",
    "            'negative_words': negative_count\n",
    "        }\n",
    "    \n",
    "    def _analyze_readability(self, sentences: List[str], words: List[str]) -> Dict:\n",
    "        \"\"\"Calculate readability metrics\"\"\"\n",
    "        if not sentences or not words:\n",
    "            return {'flesch_score': 0, 'reading_level': 'unknown'}\n",
    "        \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        \n",
    "        # Count syllables (rough approximation)\n",
    "        def count_syllables(word):\n",
    "            vowels = 'aeiouy'\n",
    "            syllables = 0\n",
    "            prev_char_vowel = False\n",
    "            \n",
    "            for char in word.lower():\n",
    "                if char in vowels:\n",
    "                    if not prev_char_vowel:\n",
    "                        syllables += 1\n",
    "                    prev_char_vowel = True\n",
    "                else:\n",
    "                    prev_char_vowel = False\n",
    "            \n",
    "            return max(1, syllables)  # Every word has at least 1 syllable\n",
    "        \n",
    "        total_syllables = sum(count_syllables(word) for word in words)\n",
    "        avg_syllables_per_word = total_syllables / len(words)\n",
    "        \n",
    "        # Flesch Reading Ease Score\n",
    "        flesch_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
    "        \n",
    "        # Determine reading level\n",
    "        if flesch_score >= 90:\n",
    "            reading_level = \"very_easy\"\n",
    "        elif flesch_score >= 80:\n",
    "            reading_level = \"easy\"\n",
    "        elif flesch_score >= 70:\n",
    "            reading_level = \"fairly_easy\"\n",
    "        elif flesch_score >= 60:\n",
    "            reading_level = \"standard\"\n",
    "        elif flesch_score >= 50:\n",
    "            reading_level = \"fairly_difficult\"\n",
    "        elif flesch_score >= 30:\n",
    "            reading_level = \"difficult\"\n",
    "        else:\n",
    "            reading_level = \"very_difficult\"\n",
    "        \n",
    "        return {\n",
    "            'flesch_score': round(flesch_score, 1),\n",
    "            'reading_level': reading_level,\n",
    "            'avg_sentence_length': round(avg_sentence_length, 1),\n",
    "            'avg_syllables_per_word': round(avg_syllables_per_word, 2)\n",
    "        }\n",
    "    \n",
    "    def _analyze_language_features(self, text: str, words: List[str]) -> Dict:\n",
    "        \"\"\"Analyze various language features\"\"\"\n",
    "        features = {\n",
    "            'lexical_diversity': 0,\n",
    "            'formality_score': 0,\n",
    "            'complexity_indicators': []\n",
    "        }\n",
    "        \n",
    "        # Lexical diversity (unique words / total words)\n",
    "        if words:\n",
    "            unique_words = set(words)\n",
    "            features['lexical_diversity'] = round(len(unique_words) / len(words), 3)\n",
    "        \n",
    "        # Formality indicators\n",
    "        formal_indicators = ['therefore', 'however', 'furthermore', 'moreover', 'consequently', \n",
    "                           'nevertheless', 'accordingly', 'thus', 'hence', 'whereas']\n",
    "        informal_indicators = ['really', 'pretty', 'quite', 'very', 'totally', 'basically', \n",
    "                             'actually', 'literally', 'obviously', 'definitely']\n",
    "        \n",
    "        formal_count = sum(1 for word in words if word in formal_indicators)\n",
    "        informal_count = sum(1 for word in words if word in informal_indicators)\n",
    "        \n",
    "        if formal_count + informal_count > 0:\n",
    "            features['formality_score'] = formal_count / (formal_count + informal_count)\n",
    "        \n",
    "        # Complexity indicators\n",
    "        if re.search(r'\\b(?:complex|complicated|sophisticated|intricate)\\b', text, re.IGNORECASE):\n",
    "            features['complexity_indicators'].append('complex_vocabulary')\n",
    "        \n",
    "        if re.search(r'[;:]', text):\n",
    "            features['complexity_indicators'].append('complex_punctuation')\n",
    "        \n",
    "        # Average word length\n",
    "        if words:\n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            if avg_word_length > 5:\n",
    "                features['complexity_indicators'].append('long_words')\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2b64a",
   "metadata": {},
   "source": [
    "### Metadata Generator\n",
    " \n",
    "- Generates structured metadata from document processing and NLP analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d5ed23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DocumentMetadata:\n",
    "    \"\"\"Structured metadata for a document\"\"\"\n",
    "    # Basic document information\n",
    "    document_id: str\n",
    "    filename: str\n",
    "    file_size: int\n",
    "    file_type: str\n",
    "    processing_date: str\n",
    "    \n",
    "    # Content analysis\n",
    "    title: Optional[str]\n",
    "    summary: str\n",
    "    language: str\n",
    "    \n",
    "    # Text statistics\n",
    "    word_count: int\n",
    "    sentence_count: int\n",
    "    paragraph_count: int\n",
    "    reading_time_minutes: int\n",
    "    \n",
    "    # Topics and keywords\n",
    "    primary_topics: List[str]\n",
    "    keywords: List[str]\n",
    "    entities: Dict[str, List[str]]\n",
    "    \n",
    "    # Document characteristics\n",
    "    document_type: str\n",
    "    formality_level: str\n",
    "    complexity_level: str\n",
    "    sentiment: str\n",
    "    \n",
    "    # Technical metadata\n",
    "    readability_score: float\n",
    "    readability_level: str\n",
    "    structure_score: float\n",
    "    \n",
    "    # Additional fields\n",
    "    tags: List[str]\n",
    "    categories: List[str]\n",
    "    confidence_score: float\n",
    "\n",
    "class MetadataGenerator:\n",
    "    \"\"\"\n",
    "    Generates structured metadata from document processing and NLP analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the metadata generator\"\"\"\n",
    "        logger.info(\"Initializing Metadata Generator...\")\n",
    "        \n",
    "        # Document type classification patterns\n",
    "        self.document_type_patterns = {\n",
    "            'research_paper': ['abstract', 'methodology', 'conclusion', 'references', 'hypothesis'],\n",
    "            'business_report': ['executive summary', 'revenue', 'quarterly', 'analysis', 'recommendations'],\n",
    "            'technical_document': ['system', 'architecture', 'implementation', 'configuration', 'api'],\n",
    "            'legal_document': ['agreement', 'contract', 'terms', 'conditions', 'liability'],\n",
    "            'manual': ['instructions', 'steps', 'procedure', 'guide', 'how to'],\n",
    "            'presentation': ['slide', 'agenda', 'overview', 'introduction', 'thank you'],\n",
    "            'article': ['article', 'author', 'published', 'journal', 'volume'],\n",
    "            'proposal': ['proposal', 'budget', 'timeline', 'objectives', 'deliverables']\n",
    "        }\n",
    "        \n",
    "        # Category mapping based on topics\n",
    "        self.category_mapping = {\n",
    "            'technology': ['Information Technology', 'Software Development', 'Digital Innovation'],\n",
    "            'business': ['Business Strategy', 'Finance', 'Marketing', 'Management'],\n",
    "            'research': ['Academic Research', 'Scientific Study', 'Data Analysis'],\n",
    "            'medical': ['Healthcare', 'Medical Research', 'Clinical Studies'],\n",
    "            'legal': ['Legal Documents', 'Contracts', 'Compliance'],\n",
    "            'education': ['Educational Content', 'Training Materials', 'Academic'],\n",
    "            'finance': ['Financial Analysis', 'Investment', 'Banking']\n",
    "        }\n",
    "        \n",
    "        logger.info(\"âœ… Metadata Generator initialized!\")\n",
    "    \n",
    "    def generate_metadata(self, \n",
    "                         document_result: Dict, \n",
    "                         nlp_result: Dict,\n",
    "                         custom_tags: List[str] = None) -> DocumentMetadata:\n",
    "        \"\"\"\n",
    "        Generate comprehensive metadata from processing results\n",
    "        \n",
    "        Args:\n",
    "            document_result: Output from DocumentProcessor\n",
    "            nlp_result: Output from NLPAnalyzer\n",
    "            custom_tags: Optional custom tags to add\n",
    "            \n",
    "        Returns:\n",
    "            DocumentMetadata object with all extracted information\n",
    "        \"\"\"\n",
    "        logger.info(f\"Generating metadata for: {document_result.get('file_metadata', {}).get('filename', 'unknown')}\")\n",
    "        \n",
    "        # Extract basic information\n",
    "        file_meta = document_result.get('file_metadata', {})\n",
    "        text_stats = document_result.get('text_statistics', {})\n",
    "        \n",
    "        # Generate unique document ID\n",
    "        doc_id = str(uuid.uuid4())[:8]\n",
    "        \n",
    "        # Extract title (heuristic approach)\n",
    "        title = self._extract_title(document_result.get('processed_text', ''))\n",
    "        \n",
    "        # Determine document type\n",
    "        doc_type = self._classify_document_type(\n",
    "            document_result.get('processed_text', ''), \n",
    "            nlp_result\n",
    "        )\n",
    "        \n",
    "        # Extract and process topics\n",
    "        topics = self._process_topics(nlp_result.get('topics', []))\n",
    "        \n",
    "        # Extract keywords\n",
    "        keywords = self._process_keywords(nlp_result.get('keywords', []))\n",
    "        \n",
    "        # Process entities\n",
    "        entities = self._process_entities(nlp_result.get('entities', {}))\n",
    "        \n",
    "        # Determine categories\n",
    "        categories = self._determine_categories(topics, entities, keywords)\n",
    "        \n",
    "        # Generate tags\n",
    "        tags = self._generate_tags(topics, keywords, doc_type, custom_tags or [])\n",
    "        \n",
    "        # Calculate scores\n",
    "        confidence_score = self._calculate_confidence_score(nlp_result)\n",
    "        structure_score = self._calculate_structure_score(nlp_result.get('document_structure', {}))\n",
    "        \n",
    "        # Determine complexity and formality\n",
    "        complexity_level = self._determine_complexity_level(nlp_result)\n",
    "        formality_level = self._determine_formality_level(nlp_result)\n",
    "        \n",
    "        # Create metadata object\n",
    "        metadata = DocumentMetadata(\n",
    "            # Basic information\n",
    "            document_id=doc_id,\n",
    "            filename=file_meta.get('filename', 'unknown'),\n",
    "            file_size=file_meta.get('file_size', 0),\n",
    "            file_type=file_meta.get('file_extension', '').upper().replace('.', ''),\n",
    "            processing_date=datetime.now().isoformat(),\n",
    "            \n",
    "            # Content\n",
    "            title=title,\n",
    "            summary=nlp_result.get('summary', ''),\n",
    "            language=text_stats.get('detected_language', 'unknown'),\n",
    "            \n",
    "            # Statistics\n",
    "            word_count=text_stats.get('word_count', 0),\n",
    "            sentence_count=text_stats.get('sentence_count', 0),\n",
    "            paragraph_count=text_stats.get('paragraph_count', 0),\n",
    "            reading_time_minutes=text_stats.get('estimated_reading_time_minutes', 0),\n",
    "            \n",
    "            # Analysis results\n",
    "            primary_topics=topics[:5],  # Top 5 topics\n",
    "            keywords=keywords[:10],     # Top 10 keywords\n",
    "            entities=entities,\n",
    "            \n",
    "            # Classification\n",
    "            document_type=doc_type,\n",
    "            formality_level=formality_level,\n",
    "            complexity_level=complexity_level,\n",
    "            sentiment=nlp_result.get('sentiment', {}).get('sentiment', 'neutral'),\n",
    "            \n",
    "            # Technical scores\n",
    "            readability_score=nlp_result.get('readability', {}).get('flesch_score', 0),\n",
    "            readability_level=nlp_result.get('readability', {}).get('reading_level', 'unknown'),\n",
    "            structure_score=structure_score,\n",
    "            \n",
    "            # Organization\n",
    "            tags=tags,\n",
    "            categories=categories,\n",
    "            confidence_score=confidence_score\n",
    "        )\n",
    "        \n",
    "        logger.info(\"âœ… Metadata generation completed\")\n",
    "        return metadata\n",
    "    \n",
    "    def _extract_title(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract document title using heuristics\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        for line in lines[:5]:  # Check first 5 lines\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 5 and len(line) < 100:\n",
    "                # Check if it looks like a title\n",
    "                if (line.isupper() or \n",
    "                    line.count(' ') < 10 or \n",
    "                    any(char in line for char in [':', '=', '-']) or\n",
    "                    not line.endswith('.')):\n",
    "                    return line\n",
    "        \n",
    "        # Fallback: use first sentence if it's short enough\n",
    "        sentences = text.split('.')\n",
    "        if sentences and len(sentences[0]) < 100:\n",
    "            return sentences[0].strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _classify_document_type(self, text: str, nlp_result: Dict) -> str:\n",
    "        \"\"\"Classify document type based on content patterns\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Score each document type\n",
    "        type_scores = {}\n",
    "        for doc_type, patterns in self.document_type_patterns.items():\n",
    "            score = sum(1 for pattern in patterns if pattern in text_lower)\n",
    "            if score > 0:\n",
    "                type_scores[doc_type] = score\n",
    "        \n",
    "        # Also consider structure\n",
    "        structure = nlp_result.get('document_structure', {})\n",
    "        if structure.get('has_title') and structure.get('has_headings'):\n",
    "            type_scores['report'] = type_scores.get('report', 0) + 2\n",
    "        \n",
    "        # Return highest scoring type or 'document' as default\n",
    "        if type_scores:\n",
    "            return max(type_scores.items(), key=lambda x: x[1])[0].replace('_', ' ').title()\n",
    "        \n",
    "        return 'Document'\n",
    "    \n",
    "    def _process_topics(self, topics: List) -> List[str]:\n",
    "        \"\"\"Process and clean topic information\"\"\"\n",
    "        processed_topics = []\n",
    "        for topic in topics:\n",
    "            if hasattr(topic, 'topic'):\n",
    "                topic_name = topic.topic\n",
    "            elif isinstance(topic, dict):\n",
    "                topic_name = topic.get('topic', '')\n",
    "            else:\n",
    "                topic_name = str(topic)\n",
    "            \n",
    "            if topic_name and topic_name.lower() != 'general':\n",
    "                processed_topics.append(topic_name.title())\n",
    "        \n",
    "        return processed_topics\n",
    "    \n",
    "    def _process_keywords(self, keywords: List) -> List[str]:\n",
    "        \"\"\"Process and clean keyword information\"\"\"\n",
    "        processed_keywords = []\n",
    "        for keyword in keywords:\n",
    "            if isinstance(keyword, tuple):\n",
    "                word = keyword[0]\n",
    "            elif isinstance(keyword, dict):\n",
    "                word = keyword.get('word', '')\n",
    "            else:\n",
    "                word = str(keyword)\n",
    "            \n",
    "            if word and len(word) > 3:\n",
    "                processed_keywords.append(word.lower())\n",
    "        \n",
    "        return processed_keywords\n",
    "    \n",
    "    def _process_entities(self, entities: Dict) -> Dict[str, List[str]]:\n",
    "        \"\"\"Process and clean entity information\"\"\"\n",
    "        processed_entities = {}\n",
    "        \n",
    "        for entity_type, entity_list in entities.items():\n",
    "            clean_entities = []\n",
    "            for entity in entity_list:\n",
    "                if hasattr(entity, 'text'):\n",
    "                    entity_text = entity.text\n",
    "                elif isinstance(entity, dict):\n",
    "                    entity_text = entity.get('text', '')\n",
    "                else:\n",
    "                    entity_text = str(entity)\n",
    "                \n",
    "                if entity_text:\n",
    "                    clean_entities.append(entity_text)\n",
    "            \n",
    "            if clean_entities:\n",
    "                processed_entities[entity_type.replace('_', ' ').title()] = clean_entities[:5]  # Limit to 5 per type\n",
    "        \n",
    "        return processed_entities\n",
    "    \n",
    "    def _determine_categories(self, topics: List[str], entities: Dict, keywords: List[str]) -> List[str]:\n",
    "        \"\"\"Determine document categories based on content analysis\"\"\"\n",
    "        categories = set()\n",
    "        \n",
    "        # Category from topics\n",
    "        for topic in topics:\n",
    "            topic_lower = topic.lower()\n",
    "            for domain, cats in self.category_mapping.items():\n",
    "                if domain in topic_lower:\n",
    "                    categories.update(cats[:2])  # Add first 2 categories\n",
    "        \n",
    "        # Category from entities\n",
    "        if 'Email' in entities or 'Phone' in entities:\n",
    "            categories.add('Contact Information')\n",
    "        if 'Organization' in entities:\n",
    "            categories.add('Organizational')\n",
    "        if 'Date' in entities:\n",
    "            categories.add('Time-Sensitive')\n",
    "        \n",
    "        # Category from keywords\n",
    "        keyword_str = ' '.join(keywords).lower()\n",
    "        if any(word in keyword_str for word in ['report', 'analysis', 'study']):\n",
    "            categories.add('Analytical')\n",
    "        if any(word in keyword_str for word in ['policy', 'procedure', 'guideline']):\n",
    "            categories.add('Procedural')\n",
    "        \n",
    "        return list(categories)[:5]  # Limit to 5 categories\n",
    "    \n",
    "    def _generate_tags(self, topics: List[str], keywords: List[str], doc_type: str, custom_tags: List[str]) -> List[str]:\n",
    "        \"\"\"Generate relevant tags for the document\"\"\"\n",
    "        tags = set()\n",
    "        \n",
    "        # Add topic-based tags\n",
    "        tags.update([topic.lower().replace(' ', '_') for topic in topics])\n",
    "        \n",
    "        # Add document type tag\n",
    "        tags.add(doc_type.lower().replace(' ', '_'))\n",
    "        \n",
    "        # Add high-frequency keyword tags\n",
    "        tags.update(keywords[:5])\n",
    "        \n",
    "        # Add custom tags\n",
    "        tags.update([tag.lower().replace(' ', '_') for tag in custom_tags])\n",
    "        \n",
    "        # Clean and return\n",
    "        clean_tags = [tag for tag in tags if len(tag) > 2 and len(tag) < 20]\n",
    "        return sorted(clean_tags)[:10]  # Limit to 10 tags\n",
    "    \n",
    "    def _calculate_confidence_score(self, nlp_result: Dict) -> float:\n",
    "        \"\"\"Calculate overall confidence score for the metadata\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Topic confidence\n",
    "        topics = nlp_result.get('topics', [])\n",
    "        if topics:\n",
    "            topic_confidences = []\n",
    "            for topic in topics:\n",
    "                if hasattr(topic, 'confidence'):\n",
    "                    topic_confidences.append(topic.confidence)\n",
    "                elif isinstance(topic, dict):\n",
    "                    topic_confidences.append(topic.get('confidence', 0.5))\n",
    "            if topic_confidences:\n",
    "                scores.append(sum(topic_confidences) / len(topic_confidences))\n",
    "        \n",
    "        # Entity confidence (pattern-based entities have high confidence)\n",
    "        entities = nlp_result.get('entities', {})\n",
    "        if entities:\n",
    "            scores.append(0.8)  # High confidence for pattern-based extraction\n",
    "        \n",
    "        # Readability confidence\n",
    "        readability = nlp_result.get('readability', {})\n",
    "        if readability.get('flesch_score', 0) > 0:\n",
    "            scores.append(0.7)\n",
    "        \n",
    "        # Structure confidence\n",
    "        structure = nlp_result.get('document_structure', {})\n",
    "        structure_indicators = sum([\n",
    "            structure.get('has_title', False),\n",
    "            structure.get('has_headings', False),\n",
    "            structure.get('sections', 0) > 0\n",
    "        ])\n",
    "        scores.append(structure_indicators / 3)\n",
    "        \n",
    "        return round(sum(scores) / len(scores) if scores else 0.5, 2)\n",
    "    \n",
    "    def _calculate_structure_score(self, structure: Dict) -> float:\n",
    "        \"\"\"Calculate document structure score\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Points for structural elements\n",
    "        if structure.get('has_title'):\n",
    "            score += 0.2\n",
    "        if structure.get('has_headings'):\n",
    "            score += 0.3\n",
    "        if structure.get('has_lists'):\n",
    "            score += 0.2\n",
    "        if structure.get('sections', 0) > 0:\n",
    "            score += 0.2\n",
    "        if structure.get('paragraphs', 0) > 1:\n",
    "            score += 0.1\n",
    "        \n",
    "        return round(min(score, 1.0), 2)\n",
    "    \n",
    "    def _determine_complexity_level(self, nlp_result: Dict) -> str:\n",
    "        \"\"\"Determine document complexity level\"\"\"\n",
    "        readability = nlp_result.get('readability', {})\n",
    "        language_features = nlp_result.get('language_features', {})\n",
    "        \n",
    "        flesch_score = readability.get('flesch_score', 50)\n",
    "        lexical_diversity = language_features.get('lexical_diversity', 0.5)\n",
    "        complexity_indicators = len(language_features.get('complexity_indicators', []))\n",
    "        \n",
    "        # Calculate complexity score\n",
    "        complexity_score = 0\n",
    "        \n",
    "        if flesch_score < 30:\n",
    "            complexity_score += 3\n",
    "        elif flesch_score < 50:\n",
    "            complexity_score += 2\n",
    "        elif flesch_score < 70:\n",
    "            complexity_score += 1\n",
    "        \n",
    "        if lexical_diversity > 0.7:\n",
    "            complexity_score += 2\n",
    "        elif lexical_diversity > 0.5:\n",
    "            complexity_score += 1\n",
    "        \n",
    "        complexity_score += complexity_indicators\n",
    "        \n",
    "        if complexity_score >= 5:\n",
    "            return 'High'\n",
    "        elif complexity_score >= 3:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    def _determine_formality_level(self, nlp_result: Dict) -> str:\n",
    "        \"\"\"Determine document formality level\"\"\"\n",
    "        language_features = nlp_result.get('language_features', {})\n",
    "        formality_score = language_features.get('formality_score', 0.5)\n",
    "        \n",
    "        if formality_score > 0.7:\n",
    "            return 'Formal'\n",
    "        elif formality_score > 0.3:\n",
    "            return 'Semi-formal'\n",
    "        else:\n",
    "            return 'Informal'\n",
    "    \n",
    "    def export_metadata(self, metadata: DocumentMetadata, format: str = 'json') -> str:\n",
    "        \"\"\"\n",
    "        Export metadata in various formats\n",
    "        \n",
    "        Args:\n",
    "            metadata: DocumentMetadata object\n",
    "            format: Export format ('json', 'xml', 'yaml')\n",
    "            \n",
    "        Returns:\n",
    "            Formatted metadata string\n",
    "        \"\"\"\n",
    "        metadata_dict = asdict(metadata)\n",
    "        \n",
    "        if format.lower() == 'json':\n",
    "            return json.dumps(metadata_dict, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        elif format.lower() == 'xml':\n",
    "            return self._dict_to_xml(metadata_dict, 'document_metadata')\n",
    "        \n",
    "        elif format.lower() == 'yaml':\n",
    "            try:\n",
    "                import yaml\n",
    "                return yaml.dump(metadata_dict, default_flow_style=False)\n",
    "            except ImportError:\n",
    "                logger.warning(\"PyYAML not installed, falling back to JSON\")\n",
    "                return json.dumps(metadata_dict, indent=2)\n",
    "        \n",
    "        else:\n",
    "            return json.dumps(metadata_dict, indent=2)\n",
    "    \n",
    "    def _dict_to_xml(self, data: Dict, root_name: str) -> str:\n",
    "        \"\"\"Convert dictionary to XML format\"\"\"\n",
    "        def dict_to_xml_recursive(d, root):\n",
    "            xml_str = f\"<{root}>\\n\"\n",
    "            for key, value in d.items():\n",
    "                if isinstance(value, dict):\n",
    "                    xml_str += f\"  {dict_to_xml_recursive(value, key)}\\n\"\n",
    "                elif isinstance(value, list):\n",
    "                    xml_str += f\"  <{key}>\\n\"\n",
    "                    for item in value:\n",
    "                        if isinstance(item, str):\n",
    "                            xml_str += f\"    <item>{item}</item>\\n\"\n",
    "                        else:\n",
    "                            xml_str += f\"    <item>{str(item)}</item>\\n\"\n",
    "                    xml_str += f\"  </{key}>\\n\"\n",
    "                else:\n",
    "                    xml_str += f\"  <{key}>{str(value)}</{key}>\\n\"\n",
    "            xml_str += f\"</{root}>\"\n",
    "            return xml_str\n",
    "        \n",
    "        return dict_to_xml_recursive(data, root_name)\n",
    "    \n",
    "    def save_metadata(self, metadata: DocumentMetadata, output_path: str, format: str = 'json'):\n",
    "        \"\"\"Save metadata to file\"\"\"\n",
    "        formatted_data = self.export_metadata(metadata, format)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(formatted_data)\n",
    "        \n",
    "        logger.info(f\"Metadata saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbc8b2",
   "metadata": {},
   "source": [
    "### Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f89e6d-6143-4f45-a026-3274cb54b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 17:49:45,994 - INFO - OCR (Tesseract) is available\n",
      "2025-06-25 17:49:45,996 - INFO - Initializing NLP Analyzer...\n",
      "2025-06-25 17:49:45,998 - INFO - âœ… NLP Analyzer initialized!\n",
      "2025-06-25 17:49:46,001 - INFO - Initializing Metadata Generator...\n",
      "2025-06-25 17:49:46,002 - INFO - âœ… Metadata Generator initialized!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Initialized components: DocumentProcessor, NLPAnalyzer, MetadataGenerator\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import JSON\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "processor = DocumentProcessor()\n",
    "analyzer = NLPAnalyzer()\n",
    "generator = MetadataGenerator()\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Initialized components: DocumentProcessor, NLPAnalyzer, MetadataGenerator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31ffe10a-cc26-4863-bd72-2c0310e2f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Current Configuration:\n",
      "  Export Format: JSON\n",
      "  Include Confidence: True\n",
      "  Batch Mode: True\n",
      "  Confidence Threshold: 0.5\n",
      "  Max Keywords: 20\n",
      "  Enable OCR: True\n",
      "  Processing Options: Deep Topic Analysis\n"
     ]
    }
   ],
   "source": [
    "#Configuration and Settings Class\n",
    "class ProcessingConfig:\n",
    "    def __init__(self):\n",
    "        self.export_format = \"JSON\"\n",
    "        self.include_confidence = True\n",
    "        self.batch_mode = True\n",
    "        self.confidence_threshold = 0.5\n",
    "        self.max_keywords = 20\n",
    "        self.enable_ocr = True\n",
    "        self.enable_entity_linking = False\n",
    "        self.processing_options = [\"Deep Topic Analysis\"]\n",
    "        self.output_language = \"Auto-detect\"\n",
    "        \n",
    "    def display_config(self):\n",
    "        print(\"ðŸ”§ Current Configuration:\")\n",
    "        print(f\"  Export Format: {self.export_format}\")\n",
    "        print(f\"  Include Confidence: {self.include_confidence}\")\n",
    "        print(f\"  Batch Mode: {self.batch_mode}\")\n",
    "        print(f\"  Confidence Threshold: {self.confidence_threshold}\")\n",
    "        print(f\"  Max Keywords: {self.max_keywords}\")\n",
    "        print(f\"  Enable OCR: {self.enable_ocr}\")\n",
    "        print(f\"  Processing Options: {', '.join(self.processing_options)}\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = ProcessingConfig()\n",
    "config.display_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4e76fa9-eaa6-47ec-8032-4bec60a9b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Processing Functions\n",
    "def process_single_file(file_path: str, custom_tags: Optional[List[str]] = None, \n",
    "                       processing_options: Optional[List[str]] = None) -> Any:\n",
    "    \"\"\"Process a single file and return metadata\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    print(f\"ðŸ“„ Processing: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Process the document\n",
    "        doc_result = processor.process_document(str(file_path))\n",
    "        print(f\"  âœ“ Document processed\")\n",
    "        \n",
    "        # Analyze with NLP\n",
    "        nlp_result = analyzer.analyze_document(doc_result['processed_text'], file_path.name)\n",
    "        print(f\"  âœ“ NLP analysis completed\")\n",
    "        \n",
    "        # Generate metadata\n",
    "        metadata = generator.generate_metadata(doc_result, nlp_result, custom_tags)\n",
    "        print(f\"  âœ“ Metadata generated\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error processing {file_path.name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_multiple_files(file_paths: List[str], custom_tags: Optional[List[str]] = None) -> List[Any]:\n",
    "    \"\"\"Process multiple files and return list of metadata\"\"\"\n",
    "    results = []\n",
    "    total_files = len(file_paths)\n",
    "    \n",
    "    print(f\"ðŸš€ Starting batch processing of {total_files} files...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths, 1):\n",
    "        print(f\"\\n[{i}/{total_files}] \", end=\"\")\n",
    "        metadata = process_single_file(file_path, custom_tags)\n",
    "        \n",
    "        if metadata:\n",
    "            results.append(metadata)\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (i / total_files) * 100\n",
    "        print(f\"Progress: {progress:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâœ… Batch processing complete! Successfully processed {len(results)}/{total_files} files\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adce26a2-c07d-41c6-9a0d-593d9f842152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics and Visualization Functions\n",
    "def create_analytics_dashboard(metadata_list: List[Any]):\n",
    "    \"\"\"Create analytics dashboard for processed documents\"\"\"\n",
    "    if not metadata_list:\n",
    "        print(\"No documents to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"ðŸ“Š Document Analytics Dashboard\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_docs = len(metadata_list)\n",
    "    avg_words = sum(m.word_count for m in metadata_list) / len(metadata_list)\n",
    "    avg_confidence = sum(m.confidence_score for m in metadata_list) / len(metadata_list)\n",
    "    total_reading_time = sum(m.reading_time_minutes for m in metadata_list)\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Key Metrics:\")\n",
    "    print(f\"  Total Documents: {total_docs}\")\n",
    "    print(f\"  Average Word Count: {int(avg_words):,}\")\n",
    "    print(f\"  Average Confidence: {avg_confidence:.2f}\")\n",
    "    print(f\"  Total Reading Time: {total_reading_time} minutes\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Document types distribution\n",
    "    doc_types = [m.document_type for m in metadata_list]\n",
    "    type_counts = pd.Series(doc_types).value_counts()\n",
    "    ax1.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "    ax1.set_title('Document Types Distribution')\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiments = [m.sentiment for m in metadata_list]\n",
    "    sentiment_counts = pd.Series(sentiments).value_counts()\n",
    "    ax2.bar(sentiment_counts.index, sentiment_counts.values, color='skyblue')\n",
    "    ax2.set_title('Sentiment Distribution')\n",
    "    ax2.set_xlabel('Sentiment')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    # Word count distribution\n",
    "    word_counts = [m.word_count for m in metadata_list]\n",
    "    ax3.hist(word_counts, bins=10, color='lightgreen', alpha=0.7)\n",
    "    ax3.set_title('Word Count Distribution')\n",
    "    ax3.set_xlabel('Word Count')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    \n",
    "    # Confidence scores\n",
    "    confidence_scores = [m.confidence_score for m in metadata_list]\n",
    "    ax4.boxplot(confidence_scores)\n",
    "    ax4.set_title('Confidence Score Distribution')\n",
    "    ax4.set_ylabel('Confidence Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_metadata_summary(metadata_list: List[Any]):\n",
    "    \"\"\"Display summary table of all metadata\"\"\"\n",
    "    if not metadata_list:\n",
    "        print(\"No documents to display\")\n",
    "        return\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for metadata in metadata_list:\n",
    "        summary_data.append({\n",
    "            'Filename': metadata.filename,\n",
    "            'Title': metadata.title,\n",
    "            'Document Type': metadata.document_type,\n",
    "            'Word Count': metadata.word_count,\n",
    "            'Confidence': f\"{metadata.confidence_score:.2f}\",\n",
    "            'Sentiment': metadata.sentiment,\n",
    "            'Top Topics': ', '.join(metadata.primary_topics[:3]),\n",
    "            'Reading Time (min)': metadata.reading_time_minutes\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    print(\"ðŸ“‹ Document Summary Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20a76490-b018-4730-916a-203be0c87e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Functions\n",
    "def export_metadata(metadata_list: List[Any], export_format: str = \"JSON\", \n",
    "                   output_dir: str = \"exports\") -> str:\n",
    "    \"\"\"Export metadata to various formats\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if export_format.upper() == \"JSON\":\n",
    "        filename = f\"metadata_export_{timestamp}.json\"\n",
    "        filepath = output_path / filename\n",
    "        \n",
    "        all_metadata = []\n",
    "        for metadata in metadata_list:\n",
    "            json_data = json.loads(generator.export_metadata(metadata, 'json'))\n",
    "            all_metadata.append(json_data)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    elif export_format.upper() == \"CSV\":\n",
    "        filename = f\"metadata_export_{timestamp}.csv\"\n",
    "        filepath = output_path / filename\n",
    "        \n",
    "        csv_data = []\n",
    "        for metadata in metadata_list:\n",
    "            csv_data.append({\n",
    "                'filename': metadata.filename,\n",
    "                'title': metadata.title,\n",
    "                'summary': metadata.summary,\n",
    "                'document_type': metadata.document_type,\n",
    "                'word_count': metadata.word_count,\n",
    "                'confidence_score': metadata.confidence_score,\n",
    "                'sentiment': metadata.sentiment,\n",
    "                'topics': '|'.join(metadata.primary_topics),\n",
    "                'keywords': '|'.join(metadata.keywords),\n",
    "                'reading_time_minutes': metadata.reading_time_minutes\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    \n",
    "    elif export_format.upper() == \"XML\":\n",
    "        filename = f\"metadata_export_{timestamp}.xml\"\n",
    "        filepath = output_path / filename\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<documents>\\n')\n",
    "            for metadata in metadata_list:\n",
    "                xml_data = generator.export_metadata(metadata, 'xml')\n",
    "                f.write(xml_data + '\\n')\n",
    "            f.write('</documents>')\n",
    "    \n",
    "    print(f\"âœ… Exported {len(metadata_list)} documents to: {filepath}\")\n",
    "    return str(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ff1ddd4-3522-467a-ac94-84e95c50e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main CLI Interface Functions\n",
    "def get_file_paths():\n",
    "    \"\"\"Interactive file selection\"\"\"\n",
    "    print(\"ðŸ“ File Selection:\")\n",
    "    print(\"1. Enter single file path\")\n",
    "    print(\"2. Enter multiple file paths (comma-separated)\")\n",
    "    print(\"3. Enter directory path (process all supported files)\")\n",
    "    \n",
    "    choice = input(\"\\nChoose option (1-3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        file_path = input(\"Enter file path: \").strip()\n",
    "        if Path(file_path).exists():\n",
    "            return [file_path]\n",
    "        else:\n",
    "            print(f\"âŒ File not found: {file_path}\")\n",
    "            return []\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        paths_input = input(\"Enter file paths (comma-separated): \").strip()\n",
    "        file_paths = [p.strip() for p in paths_input.split(',')]\n",
    "        valid_paths = []\n",
    "        for path in file_paths:\n",
    "            if Path(path).exists():\n",
    "                valid_paths.append(path)\n",
    "            else:\n",
    "                print(f\"âŒ File not found: {path}\")\n",
    "        return valid_paths\n",
    "    \n",
    "    elif choice == \"3\":\n",
    "        dir_path = input(\"Enter directory path: \").strip()\n",
    "        if Path(dir_path).is_dir():\n",
    "            supported_extensions = ['.pdf', '.docx', '.doc', '.txt']\n",
    "            file_paths = []\n",
    "            for ext in supported_extensions:\n",
    "                file_paths.extend(list(Path(dir_path).glob(f\"*{ext}\")))\n",
    "            return [str(p) for p in file_paths]\n",
    "        else:\n",
    "            print(f\"âŒ Directory not found: {dir_path}\")\n",
    "            return []\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ Invalid choice\")\n",
    "        return []\n",
    "\n",
    "def get_custom_tags():\n",
    "    \"\"\"Get custom tags from user input\"\"\"\n",
    "    tags_input = input(\"Enter custom tags (comma-separated, or press Enter to skip): \").strip()\n",
    "    if tags_input:\n",
    "        return [tag.strip() for tag in tags_input.split(',')]\n",
    "    return None\n",
    "\n",
    "def display_metadata_details(metadata_list: List[Any]):\n",
    "    \"\"\"Display detailed metadata for each document\"\"\"\n",
    "    for i, metadata in enumerate(metadata_list, 1):\n",
    "        print(f\"\\nðŸ“„ Document {i}: {metadata.filename}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Title: {metadata.title}\")\n",
    "        print(f\"Summary: {metadata.summary}\")\n",
    "        print(f\"Document Type: {metadata.document_type}\")\n",
    "        print(f\"Word Count: {metadata.word_count:,}\")\n",
    "        print(f\"Confidence Score: {metadata.confidence_score:.2f}\")\n",
    "        print(f\"Sentiment: {metadata.sentiment}\")\n",
    "        print(f\"Reading Time: {metadata.reading_time_minutes} minutes\")\n",
    "        print(f\"Primary Topics: {', '.join(metadata.primary_topics)}\")\n",
    "        print(f\"Keywords: {', '.join(metadata.keywords[:10])}\")\n",
    "        \n",
    "        if hasattr(metadata, 'entities') and metadata.entities:\n",
    "            print(\"Entities:\")\n",
    "            for entity_type, entities in metadata.entities.items():\n",
    "                if entities:\n",
    "                    print(f\"  - {entity_type}: {', '.join(entities[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d810a1d6-dce3-4f2d-9e42-4f013ffe79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing Workflow\n",
    "def run_metadata_generation():\n",
    "    \"\"\"Main workflow for metadata generation\"\"\"\n",
    "    print(\"ðŸ¤– Automated Metadata Generation System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Get file paths\n",
    "    file_paths = get_file_paths()\n",
    "    if not file_paths:\n",
    "        print(\"âŒ No valid files selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ… Found {len(file_paths)} file(s) to process\")\n",
    "    \n",
    "    # Step 2: Get custom tags\n",
    "    custom_tags = get_custom_tags()\n",
    "    if custom_tags:\n",
    "        print(f\"ðŸ·ï¸  Custom tags: {', '.join(custom_tags)}\")\n",
    "    \n",
    "    # Step 3: Process files\n",
    "    print(f\"\\nðŸš€ Starting processing...\")\n",
    "    if len(file_paths) == 1:\n",
    "        metadata_list = [process_single_file(file_paths[0], custom_tags)]\n",
    "        metadata_list = [m for m in metadata_list if m is not None]\n",
    "    else:\n",
    "        metadata_list = process_multiple_files(file_paths, custom_tags)\n",
    "    \n",
    "    if not metadata_list:\n",
    "        print(\"âŒ No documents were successfully processed.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Display results\n",
    "    print(f\"\\nðŸ“Š Processing Results:\")\n",
    "    display_metadata_summary(metadata_list)\n",
    "    \n",
    "    # Step 5: Show analytics\n",
    "    create_analytics_dashboard(metadata_list)\n",
    "    \n",
    "    # Step 6: Display detailed metadata\n",
    "    show_details = input(\"\\nðŸ” Show detailed metadata? (y/n): \").strip().lower()\n",
    "    if show_details == 'y':\n",
    "        display_metadata_details(metadata_list)\n",
    "    \n",
    "    # Step 7: Export options\n",
    "    export_choice = input(\"\\nðŸ’¾ Export results? (y/n): \").strip().lower()\n",
    "    if export_choice == 'y':\n",
    "        print(\"Export formats: JSON, CSV, XML\")\n",
    "        export_format = input(\"Choose format (JSON/CSV/XML): \").strip()\n",
    "        output_dir = input(\"Output directory (press Enter for 'exports'): \").strip() or \"exports\"\n",
    "        \n",
    "        try:\n",
    "            export_path = export_metadata(metadata_list, export_format, output_dir)\n",
    "            print(f\"âœ… Export completed: {export_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Export failed: {str(e)}\")\n",
    "    \n",
    "    return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9797fe5c-0ac7-4e00-99f8-0234f45e580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All functions loaded successfully!\n",
      "\n",
      "ðŸŽ¯ Ready to process documents!\n",
      "\n",
      "Main functions available:\n",
      "  - run_metadata_generation() : Interactive CLI workflow\n",
      "  - quick_process_file(path) : Process single file\n",
      "  - quick_process_directory(path) : Process directory\n",
      "  - quick_export(metadata_list) : Export results\n"
     ]
    }
   ],
   "source": [
    "# Quick Processing Functions (for programmatic use)\n",
    "def quick_process_file(file_path: str, custom_tags: List[str] = None) -> Any:\n",
    "    \"\"\"Quick processing of a single file - returns metadata object\"\"\"\n",
    "    return process_single_file(file_path, custom_tags)\n",
    "\n",
    "def quick_process_directory(directory_path: str, custom_tags: List[str] = None) -> List[Any]:\n",
    "    \"\"\"Quick processing of all supported files in a directory\"\"\"\n",
    "    dir_path = Path(directory_path)\n",
    "    if not dir_path.is_dir():\n",
    "        raise ValueError(f\"Directory not found: {directory_path}\")\n",
    "    \n",
    "    supported_extensions = ['.pdf', '.docx', '.doc', '.txt']\n",
    "    file_paths = []\n",
    "    for ext in supported_extensions:\n",
    "        file_paths.extend(list(dir_path.glob(f\"*{ext}\")))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"No supported files found in: {directory_path}\")\n",
    "        return []\n",
    "    \n",
    "    return process_multiple_files([str(p) for p in file_paths], custom_tags)\n",
    "\n",
    "def quick_export(metadata_list: List[Any], format: str = \"JSON\", output_file: str = None) -> str:\n",
    "    \"\"\"Quick export function\"\"\"\n",
    "    if output_file:\n",
    "        output_dir = str(Path(output_file).parent)\n",
    "        filename = Path(output_file).name\n",
    "    else:\n",
    "        output_dir = \"exports\"\n",
    "        filename = None\n",
    "    \n",
    "    return export_metadata(metadata_list, format, output_dir)\n",
    "\n",
    "print(\"âœ… All functions loaded successfully!\")\n",
    "print(\"\\nðŸŽ¯ Ready to process documents!\")\n",
    "print(\"\\nMain functions available:\")\n",
    "print(\"  - run_metadata_generation() : Interactive CLI workflow\")\n",
    "print(\"  - quick_process_file(path) : Process single file\")\n",
    "print(\"  - quick_process_directory(path) : Process directory\")\n",
    "print(\"  - quick_export(metadata_list) : Export results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee6d520c-58b9-4ab7-9e24-2bf0f91350dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Widget Interface (Optional)\n",
    "def create_interactive_interface():\n",
    "    \"\"\"Create interactive widgets for Jupyter notebook\"\"\"\n",
    "    \n",
    "    # File upload widget\n",
    "    file_upload = widgets.FileUpload(\n",
    "        accept='.pdf,.docx,.doc,.txt',\n",
    "        multiple=True,\n",
    "        description='Upload Files'\n",
    "    )\n",
    "    \n",
    "    # Tags input\n",
    "    tags_input = widgets.Text(\n",
    "        placeholder='Enter tags separated by commas',\n",
    "        description='Custom Tags:'\n",
    "    )\n",
    "    \n",
    "    # Process button\n",
    "    process_button = widgets.Button(\n",
    "        description='Process Documents',\n",
    "        button_style='primary',\n",
    "        icon='play'\n",
    "    )\n",
    "    \n",
    "    # Output area\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_process_clicked(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"ðŸš€ Processing uploaded files...\")\n",
    "            # Process uploaded files here\n",
    "            # This would need additional logic to handle uploaded files\n",
    "            print(\"âœ… Processing complete!\")\n",
    "    \n",
    "    process_button.on_click(on_process_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    interface = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>ðŸ“„ Document Metadata Generator</h3>\"),\n",
    "        file_upload,\n",
    "        tags_input,\n",
    "        process_button,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Uncomment the next line to display the interactive interface\n",
    "# display(create_interactive_interface())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf4858a-0a0c-46db-98e0-7888be275318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“– EXAMPLE USAGE:\n",
      "============================================================\n",
      "\n",
      "# Interactive CLI mode:\n",
      "run_metadata_generation()\n",
      "\n",
      "# Quick processing examples:\n",
      "metadata = quick_process_file(\"path/to/your/document.pdf\")\n",
      "metadata_list = quick_process_directory(\"path/to/your/documents/\")\n",
      "\n",
      "# Quick export:\n",
      "quick_export(metadata_list, \"JSON\", \"my_results.json\")\n",
      "\n",
      "# Analytics:\n",
      "create_analytics_dashboard(metadata_list)\n",
      "display_metadata_summary(metadata_list)\n",
      "\n",
      "\n",
      "ðŸŽ¯ Run run_metadata_generation() to start the interactive workflow!\n",
      "ðŸ“ Or use the quick_ functions for programmatic processing.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Example Usage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“– EXAMPLE USAGE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "# Interactive CLI mode:\n",
    "run_metadata_generation()\n",
    "\n",
    "# Quick processing examples:\n",
    "metadata = quick_process_file(\"path/to/your/document.pdf\")\n",
    "metadata_list = quick_process_directory(\"path/to/your/documents/\")\n",
    "\n",
    "# Quick export:\n",
    "quick_export(metadata_list, \"JSON\", \"my_results.json\")\n",
    "\n",
    "# Analytics:\n",
    "create_analytics_dashboard(metadata_list)\n",
    "display_metadata_summary(metadata_list)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Run run_metadata_generation() to start the interactive workflow!\")\n",
    "print(\"ðŸ“ Or use the quick_ functions for programmatic processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91c82f-301e-49b8-ba55-f12c020a9829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
